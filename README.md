LLM Document Processing SystemAn intelligent, two-part application that uses a Large Language Model (LLM) to process natural language queries and extract structured information from unstructured documents. The system is built with a Streamlit frontend for a user-friendly interface and a Flask backend to handle the core processing logic and API interactions.üöÄ FeaturesIntelligent Document Retrieval: Uses a Retrieval-Augmented Generation (RAG) pipeline with an in-memory vector store to find and retrieve relevant clauses from documents based on semantic understanding, not just keyword matching.Structured Output: Provides a consistent, machine-readable JSON response for each query, containing a clear decision, amount, and justification. This is crucial for integrating with downstream applications.Explicit Query Parsing: The system parses natural language queries to extract key details like age, procedure, and policy duration before processing.Flexible Document Support: Supports a variety of document types, including PDF (.pdf), Word documents (.docx), and emails (.eml).Modular Architecture: Separates the user interface (Streamlit) from the core processing logic (Flask), allowing for a more robust and scalable solution.‚öôÔ∏è Tech StackFrontendStreamlit: For building the interactive, web-based user interface.requests: For making API calls from the Streamlit frontend to the Flask backend.BackendPython with Flask: A lightweight web framework that acts as the API server, exposing endpoints for document processing and queries.LangChain: Orchestrates the RAG pipeline.Google Gemini 2.0 Flash (gemini-2.0-flash-exp): The Large Language Model used for document analysis and response generation.HuggingFace Embeddings: Used to create the numerical representations of document chunks.Pydantic: Defines the structured data model for the JSON output, ensuring consistency.üì¶ Installation & SetupFollow these steps to set up and run the project locally.1. Clone the repositoryFirst, clone your project's repository from GitHub.git clone [your-repository-url]
cd [your-project-folder]
2. Create a virtual environmentIt is highly recommended to use a virtual environment to manage dependencies.python -m venv venv

# Activate the virtual environment
# For macOS/Linux:
source venv/bin/activate

# For Windows:
venv\Scripts\activate
3. Install dependenciesInstall all the required libraries from the requirements.txt file.pip install -r requirements.txt
4. Configure your API keyThe system requires a Google Gemini API key. Create a .env file in your project's root directory and add your key.# .env
GEMINI_API_KEY=your_actual_api_key_here
5. Run the applicationsSince the project has separate frontend and backend components, you need to run both simultaneously in separate terminals.Terminal 1: Run the Flask Backendexport FLASK_APP=api.py
export FLASK_DEBUG=1
flask run --port 8000
Terminal 2: Run the Streamlit Frontendstreamlit run app.py --server.port 8501
Your Streamlit application should now open in your browser, and it will be connected to the Flask backend running in the first terminal.üñ•Ô∏è How to UseUpload a Document: Use the "Choose a file" button in the left sidebar to upload a PDF, DOCX, or EML file.Process the Document: Click the "Process Document" button to have the backend ingest the file and create the in-memory vector store.Ask a Question: Once the document is processed, type your natural language query into the text box and click "Ask Question," or select one of the example questions.View the Answer: The application will display a structured response, including the decision, amount, justification, and the source chunks used by the LLM.üåê Architecture OverviewThis project uses a client-server architecture:The Streamlit app is a lightweight frontend that sends data to and receives data from a backend API.The Flask app is the backend API that performs all the heavy-duty processing, including document ingestion, RAG, and communication with the LLM.üîó Webhooks and DeploymentThe Flask backend is designed to easily handle API integrations, including webhooks. Once this backend is deployed to a cloud service (e.g., Render, AWS, Heroku), its public URL can be used as a webhook endpoint. This allows external services to programmatically trigger document processing or queries, making the system highly extensible.
